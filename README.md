Probability & Information Theory
This class, Probability & Information Theory, introduces the mathematical fields that enable us to quantify uncertainty as well as to make predictions despite uncertainty. These fields are essential because machine learning algorithms are both trained by imperfect data and deployed into noisy, real-world scenarios they haven’t encountered before.

Through the measured exposition of theory paired with interactive examples, you’ll develop a working understanding of variables, probability distributions, metrics for assessing distributions, and graphical models. You’ll also learn how to use information theory to measure how much meaningful signal there is within some given data. The content covered in this class is itself foundational for several other classes in the Machine Learning Foundations series, especially Intro to Statistics and Optimization.

Over the course of studying this topic, you'll:

Develop an understanding of what’s going on beneath the hood of predictive statistical models and machine learning algorithms, including those used for deep learning.
Understand the appropriate variable type and probability distribution for representing a given class of data, as well as the standard techniques for assessing the relationships between distributions.
Apply information theory to quantify the proportion of valuable signal that’s present amongst the noise of a given probability distribution.
Note that this Jupyter notebook is not intended to stand alone. It is the companion code to a lecture or to videos from Jon Krohn's Machine Learning Foundations series, which offer detail on the following:

Segment 1: Introduction to Probability

What Probability Theory Is
A Brief History: Frequentists vs Bayesians
Applications of Probability to Machine Learning
Random Variables
Discrete vs Continuous Variables
Probability Mass and Probability Density Functions
Expected Value
Measures of Central Tendency: Mean, Median, and Mode
Quantiles: Quartiles, Deciles, and Percentiles
The Box-and-Whisker Plot
Measures of Dispersion: Variance, Standard Deviation, and Standard Error
Measures of Relatedness: Covariance and Correlation
Marginal and Conditional Probabilities
Independence and Conditional Independence
Segment 2: Distributions in Machine Learning

Uniform
Gaussian: Normal and Standard Normal
The Central Limit Theorem
Log-Normal
Exponential and Laplace
Binomial and Multinomial
Poisson
Mixture Distributions
Preprocessing Data for Model Input
Segment 3: Information Theory

What Information Theory Is
Self-Information
Nats, Bits and Shannons
Shannon and Differential Entropy
Kullback-Leibler Divergence
Cross-Entropy
